<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>爬虫 on 浩瀚笔记</title><link>https://blog.talimus.eu.org/tags/%E7%88%AC%E8%99%AB/</link><description>Recent content in 爬虫 on 浩瀚笔记</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Thu, 28 Mar 2024 15:40:56 +0000</lastBuildDate><atom:link href="https://blog.talimus.eu.org/tags/%E7%88%AC%E8%99%AB/index.xml" rel="self" type="application/rss+xml"/><item><title>robots规则以及阻止垃圾爬虫</title><link>https://blog.talimus.eu.org/p/215/</link><pubDate>Thu, 28 Mar 2024 15:40:56 +0000</pubDate><guid>https://blog.talimus.eu.org/p/215/</guid><description>User-agent: * Crawl-delay: 10 Disallow: /wp-admin/ Disallow: /go/ Disallow: /wp-content/plugins/ Disallow: /?s=* Disallow: /author/ Disallow: /astra-advanced-hook/ Disallow: /theme Disallow: /demo/* Disallow: /*/page/* Disallow: /*blackhole Disallow: /?blackhole Allow: /wp-admin/admin-ajax.php User-agent: YandexBot Disallow: / User-agent: DotBot Disallow: / User-agent: AhrefsBot Disallow: / User-agent: Googlebot Disallow: / User-agent: BLEXBot Disallow: / User-agent: YaK Disallow: / User-agent: PetalBot Disallow: / User-agent: MJ12bot Disallow: / Sitemap: https://www.xxx.com/sitemap.xml</description></item><item><title>反爬虫蜘蛛攻略：Apache/Nginx/PHP禁止某些User Agent抓取网站</title><link>https://blog.talimus.eu.org/p/153/</link><pubDate>Wed, 07 Feb 2024 17:09:38 +0000</pubDate><guid>https://blog.talimus.eu.org/p/153/</guid><description>昨晚打开宝塔面板后台查看，上行与下行几乎到了7000KB的速度，不到半小时，就10G流量了，开始以为是被黑了？重载系统，恢复数据后还是一样。
下载一个网站监控报表插件一看，好家伙，是谷歌蜘蛛和一些不明ip在搞我其中一个网站！！！最狠的就是谷歌蜘蛛了，疯狂在爬！
方法一：cloudflare cdn 防火墙阻止 表达式：
(http.user_agent contains &amp;ldquo;Googlebot&amp;rdquo;) or (http.user_agent contains &amp;ldquo;SemrushBot&amp;rdquo;) or (http.user_agent contains &amp;ldquo;AhrefsBot&amp;rdquo;) or (http.user_agent contains &amp;ldquo;DotBot&amp;rdquo;)
方法二：禁止指定UA（用户代理） 宝塔面板下使用方法如下：
1、找到文件目录 /www/server/nginx/conf 文件夹下面，新建一个文件agent_deny.conf
代码如下：
#禁止Scrapy等工具的抓取 if ($http_user_agent ~* (Scrapy|Curl|HttpClient)) { return 403; } #禁止指定UA及UA为空的访问 if ($http_user_agent ~* &amp;#34;FeedDemon|Indy Library|Alexa Toolbar|AskTbFXTV|AhrefsBot|DotBot|CrawlDaddy|CoolpadWebkit|Java|Feedly|UniversalFeedParser|ApacheBench|Microsoft URL Control|Swiftbot|ZmEu|oBot|jaunty|Python-urllib|lightDeckReports Bot|YYSpider|DigExt|HttpClient|MJ12bot|heritrix|Bytespider|Ezooms|Googlebot|JikeSpider|SemrushBot|^$&amp;#34; ) { return 403; } #禁止非GET|HEAD|POST方式的抓取 if ($request_method !~ ^(GET|HEAD|POST)$) { return 403; } 2、找到网站设置里面的第7行左右 写入代码：include agent_deny.conf;
如果你网站使用火车头采集发布，使用以上代码会返回403错误，发布不了的。如果想使用火车头采集发布，请使用下面的代码：
#禁止Scrapy等工具的抓取 if ($http_user_agent ~* (Scrapy|Curl|HttpClient)) { return 403; } #禁止指定UA访问。UA为空的可以访问，比如火车头可以正常发布。 if ($http_user_agent ~ &amp;#34;FeedDemon|Indy Library|Alexa Toolbar|AskTbFXTV|AhrefsBot|CrawlDaddy|CoolpadWebkit|Java|Feedly|UniversalFeedParser|ApacheBench|Microsoft URL Control|Swiftbot|ZmEu|YandexBot|jaunty|Python-urllib|lightDeckReports Bot|YYSpider|DigExt|HttpClient|MJ12bot|heritrix|Bytespider|Ezooms|Googlebot|JikeSpider|SemrushBot&amp;#34; ) { return 403; } #禁止非GET|HEAD|POST方式的抓取 if ($request_method !</description></item></channel></rss>